[
  {
    "arxiv_id_base": "2403.03222",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "data_scale": {
      "channels": 19.0,
      "datasets": [
        "TUEG",
        "MMI",
        "BCI IV 2A"
      ],
      "eeg_hours": 16000.0,
      "subjects": 69410.0
    },
    "detailed_summary": "This paper proposes a self-supervised learning framework for EEG representation learning using a state-space model (S4) architecture. The key novelty is a knowledge-guided pre-training objective that incorporates domain-specific EEG features (frequency band power) alongside traditional reconstruction loss. The model is pre-trained on a large clinical EEG corpus (69,410 recordings, ~16,000 hours) and evaluated on two downstream tasks: motor movement (MMI dataset) and motor imagery (BCI IV 2A dataset). Results show improved performance compared to transformer-based baselines while using significantly fewer parameters (~13M vs 1B).",
    "evaluation": {
      "benchmarks": [
        "MMI dataset",
        "BCI IV 2A dataset"
      ],
      "headline_results": [
        "MMI accuracy: 87.12% (knowledge-guided S4, all S4 layers trainable)",
        "BCI IV 2A accuracy: 47.99% (knowledge-guided S4, all S4 layers trainable)",
        "parameter efficiency: ~13M parameters vs 1B for transformer baselines"
      ],
      "tasks": [
        "motor movement classification",
        "motor imagery classification"
      ]
    },
    "key_points": [
      "New EEG foundation model: Knowledge-guided S4 using state-space architecture for efficient EEG representation learning.",
      "Novel knowledge-guided objective: Combines reconstruction loss with frequency band power estimation to incorporate domain knowledge.",
      "Strong empirical results: Outperforms transformer-based baselines on motor movement and motor imagery tasks while using ~100x fewer parameters."
    ],
    "limitations": [
      "Channel mismatch between pre-training (19 channels) and BCI IV 2A (22 channels) limits performance",
      "Only evaluated on two specific downstream tasks",
      "Requires engineering of appropriate handcrafted features for knowledge-guided objectives",
      "Pre-training data limited to specific clinical EEG recordings"
    ],
    "method": {
      "architecture": "State-space model (S4) with encoder-decoder framework",
      "finetuning": "Encoder frozen, time series embeddings averaged and passed to classification head",
      "objective": "Knowledge-guided pre-training combining reconstruction loss and frequency band power estimation",
      "pretraining": "500k iterations on TUEG corpus (69,410 recordings, 16,000 hours)"
    },
    "notes": "{\"chars\": 32974, \"error\": null, \"pages\": 6, \"tool\": \"pypdf\"};input_mode=fulltext;prompt_tokens=9623",
    "one_liner": "State-space model for EEG with knowledge-guided pre-training objective.",
    "open_source": {
      "code_url": null,
      "license": null,
      "weights_url": null
    },
    "paper_type": "new_model",
    "published_date": "2024-02-15",
    "tags": {
      "backbone": [
        "mamba-ssm"
      ],
      "objective": [
        "masked-reconstruction",
        "contrastive"
      ],
      "paper_type": [
        "new-model"
      ],
      "tokenization": [
        "time-patch"
      ],
      "topology": [
        "fixed-montage"
      ]
    },
    "title": "Knowledge-guided EEG Representation Learning",
    "unique_contribution": "Introduces a knowledge-guided pre-training objective for EEG that combines reconstruction loss with frequency band power estimation, achieving better performance with fewer parameters than transformer-based approaches.",
    "used_fulltext": true
  },
  {
    "arxiv_id_base": "2402.17772",
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "data_scale": {
      "channels": 14.0,
      "datasets": [
        "DREAMER",
        "Crowdsourced",
        "STEW",
        "Driver Distraction",
        "TUAB",
        "TUEV"
      ],
      "eeg_hours": null,
      "subjects": null
    },
    "detailed_summary": "EEG2Rep addresses three key challenges in EEG representation learning: low signal-to-noise ratio, wide amplitude ranges due to inter-subject variability, and lack of explicit segmentation in continuous sequences. The model introduces two core innovations: (1) predicting masked inputs in latent representation space rather than raw EEG space, and (2) using a novel semantic subsequence preserving (SSP) masking method that provides informative masked inputs. EEG2Rep significantly outperforms state-of-the-art methods on 6 diverse EEG tasks with subject variability, demonstrating improved semantic quality of representations and robustness to noise.",
    "evaluation": {
      "benchmarks": [
        "DREAMER",
        "Crowdsourced",
        "STEW",
        "Driver Distraction",
        "TUAB",
        "TUEV"
      ],
      "headline_results": [
        "58.45% average accuracy across all tasks (linear probing)",
        "60.37% average accuracy across all tasks (fine-tuning)",
        "3-7% accuracy improvement over SOTA"
      ],
      "tasks": [
        "Emotion detection",
        "Mental workload classification",
        "Eyes open/close detection",
        "Driver distraction detection",
        "Abnormal EEG classification",
        "Event detection"
      ]
    },
    "key_points": [
      "New EEG foundation model: EEG2Rep uses semantic subsequence preserving masking and latent space reconstruction to generate rich semantic representations",
      "Core method novelty: Instead of reconstructing raw EEG, EEG2Rep predicts masked inputs in abstract representation space, eliminating noise and amplitude range issues",
      "Strongest evidence: EEG2Rep significantly outperforms state-of-the-art methods on 6 diverse EEG tasks and demonstrates robustness to noise"
    ],
    "limitations": [
      "Requires substantial computational resources for training",
      "Performance depends on optimal masking ratio selection",
      "Cross-domain generalization may require larger pre-training datasets"
    ],
    "method": {
      "architecture": "Transformer-based encoder-decoder with context-driven target prediction",
      "finetuning": "Linear probing and full model fine-tuning on downstream tasks",
      "objective": "Masked reconstruction in latent representation space",
      "pretraining": "Self-supervised learning with semantic subsequence preserving masking"
    },
    "notes": "{\"chars\": 69647, \"error\": null, \"pages\": 12, \"tool\": \"pypdf\"};input_mode=fulltext;prompt_tokens=19076",
    "one_liner": "EEG2Rep is a self-supervised learning approach for EEG representation learning that uses semantic subsequence preserving masking and latent space reconstruction to generate rich semantic representations.",
    "open_source": {
      "code_url": "https://github.com/Navidfoumani/EEG2Rep",
      "license": null,
      "weights_url": null
    },
    "paper_type": "new_model",
    "published_date": "2024-02-17",
    "tags": {
      "backbone": [
        "transformer"
      ],
      "objective": [
        "masked-reconstruction"
      ],
      "paper_type": [
        "new-model"
      ],
      "tokenization": [
        "latent-tokens"
      ],
      "topology": [
        "fixed-montage"
      ]
    },
    "title": "EEG2Rep: Enhancing Self-supervised EEG Representation Through Informative Masked Inputs",
    "unique_contribution": "EEG2Rep is the first self-supervised EEG representation learning approach that predicts masked inputs in latent space using semantic subsequence preserving masking, addressing the fundamental challenges of noise, amplitude variability, and lack of segmentation in EEG data.",
    "used_fulltext": true
  }
]
