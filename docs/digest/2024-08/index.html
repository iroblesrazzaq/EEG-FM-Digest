<!doctype html>
<html><head><meta charset='utf-8'><title>EEG-FM Digest 2024-08</title>
<link rel='stylesheet' href='../../assets/style.css'></head>
<body>
  <main>
    <h1>EEG Foundation Model Digest — 2024-08</h1>
    <section class='digest-about'><h2>About This Digest</h2><p>[why i made digest, how it works]</p></section>
    <p>Top picks: 2409.00122, 2409.00101</p>
    <section><h2>new_model</h2>
    <article class='paper-card' id='2409.00101'>
      <h3><a href='http://arxiv.org/abs/2409.00101v3'>NeuroLM: A Universal Multi-task Foundation Model for Bridging the Gap between Language and EEG Signals</a></h3>
      <div class='meta'>2024-08-27 · Wei-Bang Jiang, Yansen Wang, Bao-Liang Lu, Dongsheng Li</div>
      <p><strong>Summary Highlights:</strong></p>
      <ul class='summary-points'><li>New EEG foundation model: NeuroLM unifies diverse EEG tasks within a single model through instruction tuning, leveraging LLMs to treat EEG as a foreign language.</li><li>Novel text-aligned neural tokenizer: Encodes EEG signals into discrete tokens through vector-quantized temporal-frequency prediction, enabling alignment between EEG and text embedding spaces.</li><li>Large-scale multi-channel autoregressive pre-training: Pre-trained on 25,000 hours of EEG data to learn causal representations across different EEG channels, enhancing generalization across diverse tasks.</li></ul>
      <p><strong>Unique contribution:</strong> NeuroLM is the first multi-task foundation model for EEG that unifies diverse tasks within a single model through instruction tuning, leveraging LLMs to treat EEG as a foreign language.</p>
      <details class='summary-detail'><summary>Detailed summary</summary><p>NeuroLM introduces a novel approach to EEG signal processing by integrating EEG data into a Large Language Model (LLM) framework. The model employs a text-aligned neural tokenizer that encodes EEG signals into discrete tokens through vector-quantized temporal-frequency prediction, enabling alignment between EEG and text embedding spaces. This tokenizer is trained using adversarial methods to ensure the EEG embeddings fall into the same space as text embeddings. The model then undergoes multi-channel autoregressive pre-training on a large-scale corpus of approximately 25,000 hours of EEG data, allowing it to learn causal representations across different EEG channels. Finally, multi-task instruction tuning adapts NeuroLM to various downstream tasks, demonstrating its ability to perform multiple tasks within a single model. The largest variant, NeuroLM-XL, has 1.7 billion parameters, setting a record for EEG signal processing. When evaluated on six diverse downstream datasets, NeuroLM showcases the potential of this multi-task learning paradigm, achieving performance comparable to most single-task baselines while offering the advantage of unified task handling.</p></details>
      <p class='chips'><span class='chip chip-paper_type' title='paper type'>New Model</span> <span class='chip chip-backbone' title='backbone'>Transformer</span> <span class='chip chip-objective' title='objective'>Autoregressive</span> <span class='chip chip-tokenization' title='tokenization'>Discrete Tokens</span> <span class='chip chip-topology' title='topology'>Channel Flexible</span></p>
      <p><a href='https://github.com/935963004/NeuroLM'>code</a> </p>
    </article>
    

    <article class='paper-card' id='2409.00122'>
      <h3><a href='http://arxiv.org/abs/2409.00122v1'>Brant-X: A Unified Physiological Signal Alignment Framework</a></h3>
      <div class='meta'>2024-08-28 · Daoze Zhang, Zhizhang Yuan, Junru Chen, Kerui Chen, Yang Yang</div>
      <p><strong>Summary Highlights:</strong></p>
      <ul class='summary-points'><li>New EEG foundation model alignment framework: Brant-X uses a two-level contrastive alignment strategy to model correlations between EEG and other physiological signals.</li><li>Data-efficient knowledge transfer: Leverages a pre-trained 1B-parameter EEG foundation model (Brant-2) to empower representation learning on scarce EXG data.</li><li>State-of-the-art performance: Achieves SOTA results on sleep staging, emotion recognition, freezing of gaits detection, and eye movement communication tasks.</li></ul>
      <p><strong>Unique contribution:</strong> The first unified EEG-centric alignment framework that uses a two-level contrastive alignment strategy to transfer knowledge from a large EEG foundation model to other physiological signals, enabling effective modeling of EEG-EXG correlations across diverse downstream tasks.</p>
      <details class='summary-detail'><summary>Detailed summary</summary><p>Brant-X addresses the challenge of modeling correlations between EEG and other physiological signals (EXG) by leveraging a pre-trained EEG foundation model (Brant-2) and introducing a two-level alignment framework. The first level aligns EEG and EXG patches at a fine-grained semantic scale, while the second level aligns entire sequences at a coarser scale. This approach overcomes data scarcity by transferring knowledge from the rich EEG corpus to EXG signals, and handles differences in sampling rates and signal properties through sampling augmentation and contrastive learning. Experiments on four downstream tasks—sleep stage classification, emotion recognition, freezing of gaits detection, and eye movement communication—show that Brant-X achieves state-of-the-art performance compared to both task-agnostic and task-specific baselines, demonstrating its effectiveness in unifying physiological signal modeling across diverse scenarios.</p></details>
      <p class='chips'><span class='chip chip-paper_type' title='paper type'>New Model</span> <span class='chip chip-backbone' title='backbone'>Transformer</span> <span class='chip chip-objective' title='objective'>Contrastive</span> <span class='chip chip-tokenization' title='tokenization'>Time Patch</span> <span class='chip chip-topology' title='topology'>Channel Flexible</span></p>
      <p><a href='https://github.com/zjunet/Brant-X/'>code</a> <a href='EEG foundation model weights available'>weights</a></p>
    </article>
    </section>
  </main>
  <script src='../../assets/site.js'></script>
</body></html>
