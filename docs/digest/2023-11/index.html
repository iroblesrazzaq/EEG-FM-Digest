<!doctype html>
<html><head><meta charset='utf-8'><title>EEG-FM Digest 2023-11</title>
<link rel='stylesheet' href='../../assets/style.css'></head>
<body>
  <main>
    <h1>EEG Foundation Model Digest — 2023-11</h1>
    <section class='digest-about'><h2>About This Digest</h2><p>[why i made digest, how it works]</p></section>
    <p>Top picks: 2311.03764</p>
    <section><h2>new_model</h2>
    <article class='paper-card' id='2311.03764'>
      <h3><a href='http://arxiv.org/abs/2311.03764v4'>Neuro-GPT: Towards A Foundation Model for EEG</a></h3>
      <div class='meta'>2023-11-07 · Wenhui Cui, Woojae Jeong, Philipp Thölke, Takfarinas Medani, Karim Jerbi, Anand A. Joshi, Richard M. Leahy</div>
      <p><strong>Summary Highlights:</strong></p>
      <ul class='summary-points'><li>New EEG foundation model: Neuro-GPT combines an EEG encoder with a GPT decoder for self-supervised pre-training on large-scale EEG data.</li><li>Novel masked reconstruction objective: The model learns to predict masked EEG chunks, capturing temporal correlations across different time scales.</li><li>Strong empirical validation: Fine-tuning on motor imagery classification with 9 subjects shows significant performance gains over training from scratch and previous methods.</li></ul>
      <p><strong>Unique contribution:</strong> Neuro-GPT introduces a novel foundation model architecture for EEG that combines an EEG encoder with a GPT decoder, pre-trained on large-scale EEG data via masked reconstruction to address data scarcity and heterogeneity challenges in BCI tasks.</p>
      <details class='summary-detail'><summary>Detailed summary</summary><p>Neuro-GPT addresses the scarcity and heterogeneity of EEG data for Brain-Computer Interface (BCI) tasks by proposing a foundation model consisting of an EEG encoder and a GPT model. The model is pre-trained on the large-scale TUH EEG dataset using a self-supervised masked reconstruction task, where fixed-length EEG chunks are treated as tokens and the GPT model learns to predict masked chunks. The EEG encoder, incorporating convolutional and transformer layers, extracts spatio-temporal features from raw EEG signals to provide denoised embeddings for the GPT model. The pre-trained model is then fine-tuned on a motor imagery classification task using the BCI Competition IV Dataset 2a with only 9 subjects available, demonstrating significant performance improvements compared to training from scratch and outperforming previous methods like BENDR.</p></details>
      <p class='chips'><span class='chip chip-paper_type' title='paper type'>New Model</span> <span class='chip chip-backbone' title='backbone'>Transformer</span> <span class='chip chip-objective' title='objective'>Masked Reconstruction</span> <span class='chip chip-tokenization' title='tokenization'>Time Patch</span> <span class='chip chip-topology' title='topology'>Fixed Montage</span></p>
      <p><a href='https://github.com/wenhui0206/NeuroGPT'>code</a> </p>
    </article>
    </section>
  </main>
  <script src='../../assets/site.js'></script>
</body></html>
