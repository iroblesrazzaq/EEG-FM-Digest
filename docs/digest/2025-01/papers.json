[
  {
    "arxiv_id_base": "2501.10885",
    "title": "CEReBrO: Compact Encoder for Representations of Brain Oscillations Using Efficient Alternating Attention",
    "published_date": "2025-01-18",
    "categories": [
      "cs.LG"
    ],
    "paper_type": "new_model",
    "one_liner": "CEReBrO introduces a compact EEG foundation model with alternating attention that achieves 2x speed and 6x memory improvements over standard self-attention while setting new benchmarks in emotion and seizure detection.",
    "detailed_summary": "CEReBrO addresses key limitations in current EEG foundation models by introducing a compact encoder with alternating attention mechanism that jointly models intra-channel temporal dynamics and inter-channel spatial correlations. The model uses per-channel patch tokenization and achieves 2x speed improvement with 6x less memory compared to standard self-attention. Pre-trained on over 20,000 hours of publicly available scalp EEG recordings from the Temple University EEG Corpus (TUEG) with diverse channel configurations, CEReBrO offers three model sizes (3.6M, 40M, and 85M parameters) that set new benchmarks in emotion detection and seizure detection tasks while maintaining competitive performance in anomaly classification and gait prediction. The alternating attention mechanism enables efficient processing of long EEG sequences and high-channel-count data, making it particularly suitable for deployment on resource-constrained devices.",
    "unique_contribution": "Introduces alternating attention mechanism that achieves 2x speed improvement and 6x memory reduction compared to standard self-attention while maintaining competitive performance across multiple EEG tasks.",
    "key_points": [
      "Alternating attention mechanism jointly models intra-channel temporal dynamics and inter-channel spatial correlations",
      "Achieves 2x speed improvement and 6x memory reduction compared to standard self-attention",
      "Pre-trained on 20,000+ hours of public EEG data from TUEG corpus with diverse channel configurations",
      "Three model sizes (3.6M, 40M, 85M parameters) suitable for edge deployment",
      "Sets new benchmarks in emotion detection and seizure detection tasks",
      "Uses per-channel patch tokenization for granular modeling of temporal dynamics",
      "Handles varying channel numbers through learnable padding tokens",
      "Demonstrates superior performance on public benchmarks including anomaly classification and gait prediction"
    ],
    "data_scale": {
      "datasets": [
        "TUEG",
        "TUAB",
        "SEED",
        "Neonate",
        "MoBI"
      ],
      "subjects": 10000,
      "eeg_hours": 20000,
      "channels": 64
    },
    "method": {
      "architecture": "Transformer encoder with alternating attention",
      "objective": "Masked autoencoding (MAE)",
      "pretraining": "Self-supervised on public EEG data",
      "finetuning": "Linear probing and full fine-tuning"
    },
    "evaluation": {
      "tasks": [
        "emotion detection",
        "seizure detection",
        "anomaly classification",
        "gait prediction"
      ],
      "benchmarks": [
        "SEED",
        "Neonate",
        "TUAB",
        "MoBI"
      ],
      "headline_results": [
        "68.21% accuracy (85M parameters) in emotion detection",
        "0.875 AUROC (85M parameters) in seizure detection",
        "81.67% accuracy (85M parameters) in anomaly classification",
        "0.3118 R2 score (85M parameters) in gait prediction"
      ]
    },
    "open_source": {
      "code_url": null,
      "weights_url": null,
      "license": null
    },
    "tags": {
      "paper_type": [
        "eeg-fm"
      ],
      "backbone": [
        "transformer"
      ],
      "objective": [
        "masked-reconstruction"
      ],
      "tokenization": [
        "time-patch"
      ],
      "topology": [
        "channel-flexible"
      ]
    },
    "limitations": [
      "Performance on spectrograms slightly worse than waveforms",
      "Limited to maximum 64 channels due to practical constraints",
      "No explicit comparison with other efficient attention alternatives",
      "Memory usage still significant for very long sequences",
      "No ablation study on different patch sizes"
    ],
    "used_fulltext": true,
    "notes": "{\"chars\": 60305, \"error\": null, \"pages\": null, \"tool\": \"cached\"};input_mode=fulltext;prompt_tokens=16596"
  }
]
