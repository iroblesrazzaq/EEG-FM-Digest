<!doctype html>
<html><head><meta charset='utf-8'><title>EEG-FM Digest 2025-11</title>
<link rel='stylesheet' href='../../assets/style.css'></head>
<body>
  <main>
    <h1>EEG Foundation Model Digest — 2025-11</h1>
    <section class='digest-about'><h2>About This Digest</h2><p>[why i made digest, how it works]</p></section>
    <p>Top picks: 2511.08861, 2512.08959, 2511.18571, 2511.16828, 2511.13733</p>
    <section><h2>benchmark</h2>
    <article class='paper-card' id='2512.08959'>
      <h3><a href='http://arxiv.org/abs/2512.08959v1'>EEG-Bench: A Benchmark for EEG Foundation Models in Clinical Applications</a></h3>
      <div class='meta'>2025-11-28 · Ard Kastrati, Josua Bürki, Jonas Lauer, Cheng Xuan, Raffaele Iaquinto, Roger Wattenhofer</div>
      <p><strong>Summary Highlights:</strong></p>
      <ul class='summary-points'><li>New EEG foundation model benchmark: EEG-Bench evaluates 11 clinical diagnostic tasks across 14 public datasets spanning epilepsy, schizophrenia, Parkinson&#x27;s disease, OCD, and mild traumatic brain injury.</li><li>Core method/evidence: Comprehensive comparison of classical baselines (LDA, SVM) with foundation models (BENDR, Neuro-GPT, LaBraM) shows LaBraM excels in abnormal EEG detection (0.838 balanced accuracy) while simpler models remain competitive in low-data regimes.</li><li>Main practical takeaway: Foundation models show task-dependent performance - LaBraM performs well on most tasks but struggles with epilepsy detection and sleep staging, while classical models like LDA outperform on mild traumatic brain injury classification.</li></ul>
      <p><strong>Unique contribution:</strong> First unified benchmarking framework that systematically evaluates EEG foundation models across diverse clinical tasks and datasets, revealing that simpler models can be competitive with foundation models in clinical settings.</p>
      <details class='summary-detail'><summary>Detailed summary</summary><p>This paper introduces EEG-Bench, a comprehensive benchmarking framework designed to evaluate EEG-based foundation models in clinical applications. The benchmark spans 11 well-defined diagnostic tasks across 14 publicly available EEG datasets, including epilepsy, schizophrenia, Parkinson&#x27;s disease, OCD, and mild traumatic brain injury. It features minimal preprocessing, standardized evaluation protocols, and enables side-by-side comparisons of classical baselines and modern foundation models. The authors conduct the first comprehensive comparison of EEG foundation models and classical baselines in clinical settings, finding that while foundation models achieve strong performance in certain settings, simpler models often remain competitive, particularly under clinical distribution shifts. All datasets and code are released in a reproducible, plug-and-play format to accelerate research in clinical EEG modeling.</p></details>
      <p class='chips'><span class='chip chip-paper_type' title='paper type'>Benchmark</span> <span class='chip chip-backbone' title='backbone'>Transformer</span> <span class='chip chip-objective' title='objective'>Masked Reconstruction</span> <span class='chip chip-objective' title='objective'>Contrastive</span> <span class='chip chip-tokenization' title='tokenization'>Time Patch</span> <span class='chip chip-topology' title='topology'>Fixed Montage</span> <span class='chip chip-topology' title='topology'>Channel Flexible</span></p>
      <p><a href='https://github.com/ETH-DISCO/EEG-Bench'>code</a> </p>
    </article>
    </section><section><h2>new_model</h2>
    <article class='paper-card' id='2511.13733'>
      <h3><a href='http://arxiv.org/abs/2511.13733v1'>THD-BAR: Topology Hierarchical Derived Brain Autoregressive Modeling for EEG Generic Representations</a></h3>
      <div class='meta'>2025-11-05 · Wenchao Yang, Weidong Yan, Wenkang Liu, Yulan Ma, Yang Li</div>
      <p><strong>Summary Highlights:</strong></p>
      <ul class='summary-points'><li>New EEG foundation model: THD-BAR leverages a Brain Topology Hierarchy to capture multi-scale spatial and temporal dynamics.</li><li>Novel autoregressive framework: Introduces &quot;next-scale-time prediction&quot; strategy to model complex spatio-temporal dependencies.</li><li>Extensive validation: Pre-trained on 17 datasets and evaluated on 10 downstream tasks, consistently outperforming existing methods.</li></ul>
      <p><strong>Unique contribution:</strong> THD-BAR introduces the Brain Topology Hierarchy (BTH) to redefine autoregressive learning for EEG as &quot;next-scale-time prediction,&quot; enabling simultaneous modeling of spatial and temporal dynamics.</p>
      <details class='summary-detail'><summary>Detailed summary</summary><p>THD-BAR addresses the challenge of learning universal EEG representations by introducing a Brain Topology Hierarchy (BTH) that establishes a multi-scale spatial order for EEG channels. This hierarchy enables a redefinition of autoregressive learning as &quot;next-scale-time prediction,&quot; effectively capturing both spatial and temporal dynamics. The framework includes a Topology-Hierarchical Vector Quantized-Variational Autoencoder (THVQ-VAE) for multi-scale tokenization and an enhanced Brain Autoregressive (BAR) module with specialized masking strategies. Pre-trained on 17 diverse EEG datasets and validated on 10 downstream datasets spanning 5 tasks, THD-BAR consistently outperforms existing methods, demonstrating superior generalization and modeling capabilities.</p></details>
      <p class='chips'><span class='chip chip-paper_type' title='paper type'>New Model</span> <span class='chip chip-backbone' title='backbone'>Transformer</span> <span class='chip chip-objective' title='objective'>Autoregressive</span> <span class='chip chip-tokenization' title='tokenization'>Discrete Tokens</span> <span class='chip chip-topology' title='topology'>Fixed Montage</span></p>
      <p><a href='https://github.com/thdbar/THD-BAR'>code</a> </p>
    </article>
    

    <article class='paper-card' id='2511.05863'>
      <h3><a href='http://arxiv.org/abs/2511.05863v2'>EMOD: A Unified EEG Emotion Representation Framework Leveraging V-A Guided Contrastive Learning</a></h3>
      <div class='meta'>2025-11-08 · Yuning Chen, Sha Zhao, Shijian Li, Gang Pan</div>
      <p><strong>Summary Highlights:</strong></p>
      <ul class='summary-points'><li>New EEG foundation model: EMOD learns generalizable and emotion-aware representations from heterogeneous datasets using V-A guided contrastive learning.</li><li>Unified emotion representation: Projects discrete and continuous emotion labels into shared V-A space with soft-weighted contrastive loss for semantic alignment.</li><li>Flexible architecture: Triple-Domain Encoder with Spatial-Temporal Transformer captures temporal, spectral, and spatial features across diverse EEG formats.</li></ul>
      <p><strong>Unique contribution:</strong> EMOD is the first framework to unify heterogeneous EEG emotion datasets through V-A guided contrastive learning, enabling semantically aligned representation learning across datasets with different annotation schemes and formats.</p>
      <details class='summary-detail'><summary>Detailed summary</summary><p>EMOD addresses the challenge of generalizing EEG-based emotion recognition across heterogeneous datasets by proposing a unified pretraining framework. The key innovation is projecting diverse emotion labels (discrete and continuous) into a shared Valence-Arousal space and using soft-weighted supervised contrastive learning to encourage emotionally similar samples to cluster in the latent space. To handle varying EEG formats, EMOD employs a flexible backbone with a Triple-Domain Encoder (temporal, spectral, spatial) followed by a Spatial-Temporal Transformer. The model is pretrained on 8 public EEG datasets and evaluated on 3 benchmark datasets, achieving state-of-the-art performance while using significantly fewer parameters than existing foundation models.</p></details>
      <p class='chips'><span class='chip chip-paper_type' title='paper type'>New Model</span> <span class='chip chip-backbone' title='backbone'>Transformer</span> <span class='chip chip-objective' title='objective'>Contrastive</span> <span class='chip chip-tokenization' title='tokenization'>Time Patch</span> <span class='chip chip-topology' title='topology'>Channel Flexible</span></p>
      <p><a href='https://github.com/cyn4396/EMOD'>code</a> </p>
    </article>
    

    <article class='paper-card' id='2511.08444'>
      <h3><a href='http://arxiv.org/abs/2511.08444v1'>One Model for All: Universal Pre-training for EEG based Emotion Recognition across Heterogeneous Datasets and Paradigms</a></h3>
      <div class='meta'>2025-11-11 · Xiang Li, You Li, Yazhou Zhang</div>
      <p><strong>Summary Highlights:</strong></p>
      <ul class='summary-points'><li>New EEG foundation model: &#x27;One Model for All&#x27; framework achieves SOTA performance (99.27% SEED, 93.69% DEAP, 93.93% DREAMER) through universal pre-training.</li><li>Core method novelty: Decoupled learning with univariate pre-training via contrastive learning and multivariate fine-tuning using ART-GAT architecture.</li><li>Strongest evidence: Universal pre-training prevents training collapse and provides +7.65% gain on DEAP vs scratch, with GAT module critical for handling high-noise data (+22.19% over GCN).</li></ul>
      <p><strong>Unique contribution:</strong> The paper introduces a decoupled pre-training and fine-tuning paradigm with a Unified Channel Schema that enables effective universal pre-training across heterogeneous EEG datasets, achieving SOTA performance and cross-dataset transfer.</p>
      <details class='summary-detail'><summary>Detailed summary</summary><p>This paper addresses the challenge of profound dataset heterogeneity in EEG-based emotion recognition, where varying channel configurations and subject variability hinder generalizable models. The authors propose &#x27;One Model for All&#x27;, a universal pre-training framework that decouples learning into two stages: (1) univariate pre-training via self-supervised contrastive learning on individual channels, enabled by a Unified Channel Schema (UCS) that leverages the union of channels across datasets; and (2) multivariate fine-tuning using a novel &#x27;ART&#x27; (Adaptive Resampling Transformer) and &#x27;GAT&#x27; (Graph Attention Network) architecture to capture complex spatio-temporal dependencies. Experiments demonstrate that universal pre-training is essential for stabilizing training and achieving substantial performance gains, with the framework achieving new state-of-the-art results on SEED (99.27%), DEAP (93.69%), and DREAMER (93.93%), as well as superior cross-dataset transfer performance.</p></details>
      <p class='chips'><span class='chip chip-paper_type' title='paper type'>New Model</span> <span class='chip chip-backbone' title='backbone'>Transformer</span> <span class='chip chip-objective' title='objective'>Contrastive</span> <span class='chip chip-tokenization' title='tokenization'>Time Patch</span> <span class='chip chip-topology' title='topology'>Channel Flexible</span></p>
      <p> </p>
    </article>
    

    <article class='paper-card' id='2511.08861'>
      <h3><a href='http://arxiv.org/abs/2511.08861v1'>EEG-X: Device-Agnostic and Noise-Robust Foundation Model for EEG</a></h3>
      <div class='meta'>2025-11-12 · Navid Mohammadi Foumani, Soheila Ghane, Nam Nguyen, Mahsa Salehi, Geoffrey I. Webb, Geoffrey Mackellar</div>
      <p><strong>Summary Highlights:</strong></p>
      <ul class='summary-points'><li>New EEG foundation model: EEG-X achieves state-of-the-art performance across diverse tasks and datasets through device-agnostic and noise-robust representation learning.</li><li>Novel location-based channel embedding: Encodes electrode positions and their neighborhoods, preserving brain-region similarity and enabling robust transfer across devices with different channel layouts.</li><li>Noise-aware reconstruction with DiCT: Reconstructs artifact-removed signals in both raw and latent spaces using a dictionary-inspired convolutional transformation layer that reduces noise sensitivity and captures frequency- and shape-aware similarities.</li></ul>
      <p><strong>Unique contribution:</strong> EEG-X introduces a device-agnostic and noise-robust foundation model for EEG that combines location-based channel embeddings, noise-aware reconstruction in both raw and latent spaces, and a dictionary-inspired convolutional transformation layer to achieve state-of-the-art performance across diverse EEG tasks and datasets.</p>
      <details class='summary-detail'><summary>Detailed summary</summary><p>EEG-X addresses two fundamental challenges in EEG analysis: variability across datasets caused by differences in recording devices and configurations, and the low signal-to-noise ratio (SNR) of EEG signals. The model introduces a novel location-based channel embedding that encodes spatial information and improves generalization across domains by allowing the model to handle varying channel numbers, combinations, and recording lengths. To enhance robustness against noise, EEG-X employs a noise-aware masking and reconstruction strategy in both raw and latent spaces, reconstructing denoised signals obtained through artifact removal rather than raw noisy signals. Additionally, EEG-X introduces a dictionary-inspired convolutional transformation (DiCT) layer that projects signals into a structured feature space before computing reconstruction loss, reducing noise sensitivity and capturing frequency- and shape-aware similarities. Experiments on datasets collected from diverse devices show that EEG-X outperforms state-of-the-art methods across multiple downstream EEG tasks and excels in cross-domain settings where pre-trained and downstream datasets differ in electrode layouts.</p></details>
      <p class='chips'><span class='chip chip-paper_type' title='paper type'>New Model</span> <span class='chip chip-backbone' title='backbone'>Transformer</span> <span class='chip chip-objective' title='objective'>Masked Reconstruction</span> <span class='chip chip-tokenization' title='tokenization'>Time Patch</span> <span class='chip chip-topology' title='topology'>Channel Flexible</span></p>
      <p><a href='https://github.com/Emotiv/EEG-X'>code</a> </p>
    </article>
    

    <article class='paper-card' id='2511.11940'>
      <h3><a href='http://arxiv.org/abs/2511.11940v1'>Learning the relative composition of EEG signals using pairwise relative shift pretraining</a></h3>
      <div class='meta'>2025-11-14 · Christopher Sandino, Sayeri Lala, Geeling Chau, Melika Ayoughi, Behrooz Mahasseni, Ellen Zippi, Ali Moin, Erdrin Azemi, Hanlin Goh</div>
      <p><strong>Summary Highlights:</strong></p>
      <ul class='summary-points'><li>New EEG foundation model: PARS pretraining predicts relative temporal shifts between randomly sampled EEG window pairs to capture long-range dependencies.</li><li>Method novelty: Uses cross-attention decoder to estimate pairwise temporal distances while masking positional information to avoid trivial solutions.</li><li>Strongest evidence: PARS consistently outperforms MAE, MP3, and DropPos across sleep staging, abnormal detection, seizure detection, and motor imagery tasks in both label-efficient and transfer learning settings.</li></ul>
      <p><strong>Unique contribution:</strong> PARS pretraining introduces a novel pretext task that predicts relative temporal shifts between EEG window pairs, enabling transformers to capture long-range dependencies and relative temporal composition better than existing masked reconstruction approaches.</p>
      <details class='summary-detail'><summary>Detailed summary</summary><p>Self-supervised learning (SSL) for EEG has largely relied on masked reconstruction strategies like MAE, which capture local temporal patterns but miss long-range dependencies. This paper introduces PARS (Pairwise Relative Shift) pretraining, a novel pretext task that predicts relative temporal shifts between randomly sampled EEG window pairs. Unlike reconstruction-based methods, PARS encourages encoders to learn representations that capture relative temporal composition and long-range dependencies inherent in neural signals. Through comprehensive evaluation on various EEG decoding tasks, PARS-pretrained transformers consistently outperform existing pretraining strategies in both label-efficient and transfer learning settings, establishing a new paradigm for self-supervised EEG representation learning.</p></details>
      <p class='chips'><span class='chip chip-paper_type' title='paper type'>New Model</span> <span class='chip chip-backbone' title='backbone'>Transformer</span> <span class='chip chip-objective' title='objective'>Contrastive</span> <span class='chip chip-tokenization' title='tokenization'>Time Patch</span> <span class='chip chip-topology' title='topology'>Fixed Montage</span></p>
      <p> </p>
    </article>
    

    <article class='paper-card' id='2511.16828'>
      <h3><a href='http://arxiv.org/abs/2511.16828v1'>ManifoldFormer: Geometric Deep Learning for Neural Dynamics on Riemannian Manifolds</a></h3>
      <div class='meta'>2025-11-20 · Yihang Fu, Lifang He, Qingyu Chen</div>
      <p><strong>Summary Highlights:</strong></p>
      <ul class='summary-points'><li>New EEG foundation model: ManifoldFormer introduces geometric deep learning by modeling neural signals on Riemannian manifolds rather than Euclidean space.</li><li>Novel geometric architecture: Integrates Riemannian VAE for manifold embedding, geometric Transformer with geodesic-aware attention, and neural ODE dynamics predictor.</li><li>Strong empirical results: Achieves 4.6-4.8% higher accuracy and 6.2-10.2% higher Cohen&#x27;s Kappa across four public EEG datasets compared to state-of-the-art methods.</li></ul>
      <p><strong>Unique contribution:</strong> ManifoldFormer is the first EEG foundation model to explicitly incorporate Riemannian manifold geometry into its architecture, replacing Euclidean assumptions with geodesic-aware mechanisms that better capture the intrinsic structure of neural dynamics.</p>
      <details class='summary-detail'><summary>Detailed summary</summary><p>ManifoldFormer addresses a fundamental limitation in existing EEG foundation models by explicitly modeling neural signals on Riemannian manifolds rather than treating them as generic Euclidean time series. The architecture integrates three cascaded innovations: a Riemannian VAE that learns compact manifold embeddings while preserving geometric structure through hypersphere and hyperbolic projections, a geometric Transformer with geodesic-aware attention mechanisms that operate directly on neural manifolds using manifold distances instead of Euclidean distances, and a dynamics predictor leveraging neural ODEs with manifold constraints to model smooth neural state evolution. Extensive evaluation across four public datasets demonstrates substantial improvements over state-of-the-art methods, with 4.6-4.8% higher accuracy and 6.2-10.2% higher Cohen&#x27;s Kappa, while maintaining robust cross-subject generalization. The geometric approach reveals meaningful neural patterns consistent with neurophysiological principles, establishing geometric constraints as essential for effective EEG foundation models.</p></details>
      <p class='chips'><span class='chip chip-paper_type' title='paper type'>New Model</span> <span class='chip chip-backbone' title='backbone'>Transformer</span> <span class='chip chip-objective' title='objective'>Contrastive</span> <span class='chip chip-tokenization' title='tokenization'>Time Patch</span> <span class='chip chip-topology' title='topology'>Channel Flexible</span></p>
      <p> </p>
    </article>
    

    <article class='paper-card' id='2511.18571'>
      <h3><a href='http://arxiv.org/abs/2511.18571v1'>SAMBA: Toward a Long-Context EEG Foundation Model via Spatial Embedding and Differential Mamba</a></h3>
      <div class='meta'>2025-11-23 · Jiazhen Hong, Geoffrey Mackellar, Soheila Ghane</div>
      <p><strong>Summary Highlights:</strong></p>
      <ul class='summary-points'><li>New EEG foundation model: SAMBA leverages Mamba-based architecture to handle long EEG sequences efficiently while maintaining low memory usage.</li><li>Spatial and temporal compatibility: SAMBA&#x27;s 3D Spatial-Adaptive Input Embedding enables generalization across different electrode montages, and its hierarchical design supports sequences from short to very long durations.</li><li>Superior performance: SAMBA consistently outperforms state-of-the-art methods across thirteen EEG datasets while demonstrating strong representation learnability and cross-domain transferability.</li></ul>
      <p><strong>Unique contribution:</strong> SAMBA introduces a Mamba-based U-shaped encoder-decoder architecture with Temporal Semantic Random Masking, Multi-Head Differential Mamba, and Spatial-Adaptive Input Embedding to enable efficient long-sequence EEG modeling across diverse electrode montages and recording durations.</p>
      <details class='summary-detail'><summary>Detailed summary</summary><p>SAMBA addresses the challenge of long-sequence EEG modeling by introducing a Mamba-based U-shaped encoder-decoder architecture that captures long-range temporal dependencies and spatial variability in EEG data. The model incorporates three key innovations: Temporal Semantic Random Masking for semantic-level sequence reconstruction, a Multi-Head Differential Mamba module to suppress redundancy and emphasize salient temporal structures, and a Spatial-Adaptive Input Embedding that learns unified embeddings in a three-dimensional Euclidean space, enabling robustness across devices. Experiments on thirteen EEG datasets across diverse tasks, electrode configurations, and sequence durations demonstrate that SAMBA consistently outperforms state-of-the-art methods while maintaining low memory consumption and inference time.</p></details>
      <p class='chips'><span class='chip chip-paper_type' title='paper type'>New Model</span> <span class='chip chip-backbone' title='backbone'>Mamba-SSM</span> <span class='chip chip-objective' title='objective'>Masked Reconstruction</span> <span class='chip chip-tokenization' title='tokenization'>Time Patch</span> <span class='chip chip-topology' title='topology'>Channel Flexible</span></p>
      <p><a href='https://github.com/Jiazhen-Hong/SAMBA'>code</a> </p>
    </article>
    </section>
  </main>
  <script src='../../assets/site.js'></script>
</body></html>
