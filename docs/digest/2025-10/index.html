<!doctype html>
<html><head><meta charset='utf-8'><title>EEG-FM Digest 2025-10</title>
<link rel='stylesheet' href='../../assets/style.css'></head>
<body>
  <main>
    <h1>EEG Foundation Model Digest — 2025-10</h1>
    <section class='digest-about'><h2>About This Digest</h2><p>[why i made digest, how it works]</p></section>
    <p>Top picks: 2510.22257, 2510.21585, 2510.12515, 2510.27522, 2510.16548</p>
    <section><h2>new_model</h2>
    <article class='paper-card' id='2510.08059'>
      <h3><a href='http://arxiv.org/abs/2510.08059v2'>Mitigating Subject Dependency in EEG Decoding with Subject-Specific Low-Rank Adapters</a></h3>
      <div class='meta'>2025-10-09 · Timon Klein, Piotr Minakowski, Sebastian Sager, Steffen Schotthöfer</div>
      <p><strong>Summary Highlights:</strong></p>
      <ul class='summary-points'><li>New EEG foundation model: SuLoRA addresses subject-specific distribution shifts by decomposing network weights into shared and subject-specific components.</li><li>Core method/evidence: SuLoRA achieves superior performance on MEG speech perception and EEG motor imagery tasks while using fewer parameters than baseline approaches.</li><li>Main practical takeaway: SuLoRA enables existing architectures to become robust to subject shifts without architectural redesign, offering a practical path towards effective cross-subject foundation models.</li></ul>
      <p><strong>Unique contribution:</strong> We propose SuLoRA, a novel adaptive layer that explicitly separates shared knowledge from subject-specific signatures, designed as a drop-in replacement for linear and convolutional layers in any architecture.</p>
      <details class='summary-detail'><summary>Detailed summary</summary><p>Subject-specific distribution shifts pose a fundamental challenge to developing foundation models for brain decoding. We introduce the Subject-Specific Low-Rank Adapter (SuLoRA), a drop-in replacement for standard linear or convolutional layers that decomposes weights into a shared, subject-invariant component and a lightweight, low-rank correction unique to each subject. This explicit separation enables existing architectures to become robust to subject shifts without architectural redesign. We evaluate SuLoRA on MEG speech perception and EEG motor imagery tasks across CNN and transformer architectures. In the speech decoding task, SuLoRA exceeds baseline performance with half the parameters. On motor imagery datasets, SuLoRA outperforms both subject-agnostic models and independently trained subject-specific models. SuLoRA offers a practical path towards effective cross-subject foundation models for brain signal applications.</p></details>
      <p class='chips'><span class='chip chip-paper_type' title='paper type'>New Model</span> <span class='chip chip-backbone' title='backbone'>Transformer</span> <span class='chip chip-objective' title='objective'>Contrastive</span> <span class='chip chip-tokenization' title='tokenization'>Time Patch</span> <span class='chip chip-topology' title='topology'>Channel Flexible</span></p>
      <p><a href='https://github.com/username/sulora'>code</a> </p>
    </article>
    

    <article class='paper-card' id='2510.09095'>
      <h3><a href='http://arxiv.org/abs/2510.09095v1'>Neural Codecs as Biosignal Tokenizers</a></h3>
      <div class='meta'>2025-10-10 · Kleanthis Avramidis, Tiantian Feng, Woojae Jeong, Jihwan Lee, Wenhui Cui, Richard M Leahy, Shrikanth Narayanan</div>
      <p><strong>Summary Highlights:</strong></p>
      <ul class='summary-points'><li>New EEG foundation model: BioCodec uses neural codec principles with Residual Vector Quantization to tokenize biosignals into discrete latent sequences without artificial temporal boundaries.</li><li>Competitive performance with compression: Achieves state-of-the-art results on clinical, sleep, motor imagery, and speech decoding tasks while compressing signals 8× and using fewer parameters than comparable models.</li><li>Robust low-resource learning: Maintains strong performance when trained on as little as 10-30% of available data, demonstrating the effectiveness of pre-trained codec representations for downstream adaptation.</li></ul>
      <p><strong>Unique contribution:</strong> BioCodec introduces a codec-based foundation model for biosignals that tokenizes continuous waveforms using Residual Vector Quantization, achieving competitive performance with 8× compression and fewer parameters than state-of-the-art models.</p>
      <details class='summary-detail'><summary>Detailed summary</summary><p>BioCodec is a self-supervised representation learning framework for biosignals that draws inspiration from neural audio codecs. It uses Residual Vector Quantization (RVQ) to tokenize continuous EEG and EMG waveforms into discrete latent sequences without imposing artificial temporal boundaries or multi-channel embeddings. Pre-trained on thousands of EEG hours from the TUH-EEG corpus and extended to EMG using the emg2qwerty dataset, BioCodec demonstrates efficacy across diverse downstream tasks including clinical abnormality detection, sleep staging, motor imagery, and speech decoding. The model achieves competitive or superior performance compared to state-of-the-art approaches while operating with substantially fewer parameters and supporting 8× signal compression. Qualitative analyses of codebook usage and spatial coherence validate the learned representations, and the framework shows robustness in low-resource settings.</p></details>
      <p class='chips'><span class='chip chip-paper_type' title='paper type'>New Model</span> <span class='chip chip-backbone' title='backbone'>Transformer</span> <span class='chip chip-objective' title='objective'>Discrete Code Prediction</span> <span class='chip chip-tokenization' title='tokenization'>Discrete Tokens</span> <span class='chip chip-topology' title='topology'>Channel Flexible</span></p>
      <p><a href='https://github.com/usc-sail/BioCodec'>code</a> <a href='https://github.com/usc-sail/BioCodec'>weights</a></p>
    </article>
    

    <article class='paper-card' id='2510.12515'>
      <h3><a href='http://arxiv.org/abs/2510.12515v1'>HEAR: An EEG Foundation Model with Heterogeneous Electrode Adaptive Representation</a></h3>
      <div class='meta'>2025-10-14 · Zhige Chen, Chengxuan Qin, Wenlong You, Rui Liu, Congying Chu, Rui Yang, Kay Chen Tan, Jibin Wu</div>
      <p><strong>Summary Highlights:</strong></p>
      <ul class='summary-points'><li>New EEG foundation model: HEAR is the first model explicitly designed to support heterogeneous EEG devices with varying electrode layouts and counts.</li><li>Coordinate-based spatial embedding: Uses learnable spatial embeddings to map diverse electrode configurations into a unified representational space.</li><li>Spatially-guided transformer: Processes unified spatial representations to effectively capture spatiotemporal dependencies across electrodes.</li></ul>
      <p><strong>Unique contribution:</strong> HEAR is the first EEG foundation model explicitly designed to support heterogeneous EEG devices with varying electrode layouts and counts, using a coordinate-based spatial embedding and spatially-guided transformer architecture.</p>
      <details class='summary-detail'><summary>Detailed summary</summary><p>HEAR introduces a novel EEG foundation model that addresses the challenge of heterogeneous electrode configurations across different EEG devices. The model employs a learnable, coordinate-based spatial embedding to map electrodes with varying layouts and counts into a unified representational space. This unified spatial representation is processed by a spatially-guided transformer that captures spatiotemporal dependencies across electrodes. To support HEAR&#x27;s development, the authors constructed a large-scale dataset comprising 8,782 hours of EEG data from over 150 distinct electrode layouts with up to 1,132 electrodes. Experimental results demonstrate that HEAR substantially outperforms existing EEG foundation models in supporting heterogeneous EEG devices and generalizing across diverse cognitive tasks and subjects.</p></details>
      <p class='chips'><span class='chip chip-paper_type' title='paper type'>New Model</span> <span class='chip chip-backbone' title='backbone'>Transformer</span> <span class='chip chip-objective' title='objective'>Masked Reconstruction</span> <span class='chip chip-tokenization' title='tokenization'>Time Patch</span> <span class='chip chip-topology' title='topology'>Channel Flexible</span></p>
      <p> </p>
    </article>
    

    <article class='paper-card' id='2510.13068'>
      <h3><a href='http://arxiv.org/abs/2510.13068v3'>NeuroRVQ: Multi-Scale EEG Tokenization for Generative Large Brainwave Models</a></h3>
      <div class='meta'>2025-10-15 · Konstantinos Barmpas, Na Lee, Alexandros Koliousis, Yannis Panagakis, Dimitrios A. Adamos, Nikolaos Laskaris, Stefanos Zafeiriou</div>
      <p><strong>Summary Highlights:</strong></p>
      <ul class='summary-points'><li>New EEG foundation model: NeuroRVQ with multi-scale RVQ codebook tokenizer for efficient and accurate EEG signal reconstruction.</li><li>Core method/evidence: Achieves up to 15% higher performance on five BCI downstream tasks compared to existing EEG foundation models.</li><li>Main practical takeaway: The tokenizer&#x27;s design principles can be extended to other biosignal modalities, enabling broader applications of foundation models in biosignal analysis.</li></ul>
      <p><strong>Unique contribution:</strong> NeuroRVQ introduces a novel multi-scale RVQ codebook tokenizer with a unit circle phase-aware loss, enabling state-of-the-art EEG signal reconstruction and downstream task performance.</p>
      <details class='summary-detail'><summary>Detailed summary</summary><p>NeuroRVQ addresses the challenge of EEG signal tokenization for foundation models by introducing a multi-scale RVQ codebook tokenizer. The tokenizer integrates multi-scale feature extraction with hierarchical residual vector quantization codebooks and a unit circle phase-aware loss function. This design enables efficient EEG compression and accurate reconstruction across all frequency bands, supporting robust generative masked modeling. The model is evaluated on both in-distribution and out-of-distribution datasets, demonstrating superior reconstruction performance compared to existing methods. Additionally, NeuroRVQ achieves up to 15% higher performance on five BCI downstream tasks compared to other EEG foundation models, validating the effectiveness of its codebook-based approach.</p></details>
      <p class='chips'><span class='chip chip-paper_type' title='paper type'>New Model</span> <span class='chip chip-backbone' title='backbone'>Transformer</span> <span class='chip chip-objective' title='objective'>Masked Reconstruction</span> <span class='chip chip-tokenization' title='tokenization'>Discrete Tokens</span> <span class='chip chip-topology' title='topology'>Channel Flexible</span></p>
      <p> </p>
    </article>
    

    <article class='paper-card' id='2510.16548'>
      <h3><a href='http://arxiv.org/abs/2510.16548v1'>NeurIPT: Foundation Model for Neural Interfaces</a></h3>
      <div class='meta'>2025-10-18 · Zitao Fang, Chenxuan Li, Hongting Zhou, Shuyang Yu, Guodong Du, Ashwaq Qasem, Yang Lu, Jing Li, Junsong Zhang, Sim Kuan Goh</div>
      <p><strong>Summary Highlights:</strong></p>
      <ul class='summary-points'><li>New EEG foundation model: NeurIPT achieves state-of-the-art performance across eight diverse BCI datasets including seizure detection, cognitive state decoding, and motor imagery tasks.</li><li>Novel amplitude-aware masking: AAMP masks based on signal amplitude rather than random intervals, enabling robust feature learning across varying signal intensities and avoiding trivial local interpolation.</li><li>Progressive mixture-of-experts: PMoE architecture progressively introduces specialized expert subnetworks at deeper layers to effectively capture diverse temporal dynamics in EEG signals.</li></ul>
      <p><strong>Unique contribution:</strong> NeurIPT introduces amplitude-aware masking and progressive mixture-of-experts architecture specifically designed for EEG foundation models, achieving state-of-the-art performance across eight diverse BCI tasks while addressing the unique challenges of EEG signal variability and electrode configuration differences.</p>
      <details class='summary-detail'><summary>Detailed summary</summary><p>NeurIPT addresses the challenge of applying foundation models to EEG data, which is complicated by inter-subject variability, diverse electrode configurations, and heterogeneous signal patterns. The model introduces Amplitude-Aware Masked Pretraining (AAMP), which masks segments based on signal amplitude rather than random intervals, enabling learning of robust features across varying signal intensities. A Progressive Mixture-of-Experts (PMoE) architecture progressively introduces specialized expert subnetworks at deeper layers to adapt to diverse temporal EEG patterns. Spatially, NeurIPT leverages 3D electrode coordinates for transferable embeddings and employs Intra-Inter Lobe Pooling (IILP) during fine-tuning to exploit regional brain features. Empirical evaluations across eight downstream BCI datasets demonstrate consistent state-of-the-art performance, highlighting broad applicability and robust generalization.</p></details>
      <p class='chips'><span class='chip chip-paper_type' title='paper type'>New Model</span> <span class='chip chip-backbone' title='backbone'>Transformer</span> <span class='chip chip-backbone' title='backbone'>MoE</span> <span class='chip chip-objective' title='objective'>Masked Reconstruction</span> <span class='chip chip-tokenization' title='tokenization'>Time Patch</span> <span class='chip chip-topology' title='topology'>Channel Flexible</span></p>
      <p> </p>
    </article>
    

    <article class='paper-card' id='2510.21585'>
      <h3><a href='http://arxiv.org/abs/2510.21585v1'>REVE: A Foundation Model for EEG -- Adapting to Any Setup with Large-Scale Pretraining on 25,000 Subjects</a></h3>
      <div class='meta'>2025-10-24 · Yassine El Ouahidi, Jonathan Lys, Philipp Thölke, Nicolas Farrugia, Bastien Pasdeloup, Vincent Gripon, Karim Jerbi, Giulia Lioi</div>
      <p><strong>Summary Highlights:</strong></p>
      <ul class='summary-points'><li>New EEG foundation model: REVE achieves state-of-the-art results on 10 downstream tasks including motor imagery, seizure detection, and sleep staging through large-scale pretraining on 25,000 subjects.</li><li>Novel 4D positional encoding: Introduces a Fourier-based encoding scheme that processes arbitrary electrode configurations and temporal lengths without requiring fixed montages.</li><li>Strong generalization: Demonstrates up to 17% gains in linear probing and transfers effectively to unseen electrode setups and longer inputs than used in pretraining.</li></ul>
      <p><strong>Unique contribution:</strong> REVE introduces a 4D Fourier positional encoding that natively supports arbitrary electrode layouts and sequence lengths, enabling the first EEG foundation model to generalize across diverse setups without requiring fixed montages or extensive fine-tuning.</p>
      <details class='summary-detail'><summary>Detailed summary</summary><p>REVE (Representation for EEG with Versatile Embeddings) addresses the challenge of EEG heterogeneity by introducing a novel 4D positional encoding scheme that enables processing of signals with arbitrary electrode configurations and temporal lengths. The model is pretrained on over 60,000 hours of EEG data from 92 diverse datasets spanning 25,000 subjects using a masked autoencoding objective. REVE achieves state-of-the-art performance across 10 downstream EEG tasks including motor imagery, seizure detection, sleep staging, and emotion recognition, demonstrating strong generalization with minimal fine-tuning requirements.</p></details>
      <p class='chips'><span class='chip chip-paper_type' title='paper type'>New Model</span> <span class='chip chip-backbone' title='backbone'>Transformer</span> <span class='chip chip-objective' title='objective'>Masked Reconstruction</span> <span class='chip chip-tokenization' title='tokenization'>Time Patch</span> <span class='chip chip-topology' title='topology'>Channel Flexible</span></p>
      <p><a href='https://github.com/reve-model/reve'>code</a> <a href='https://huggingface.co/reve-model/reve'>weights</a></p>
    </article>
    

    <article class='paper-card' id='2510.22257'>
      <h3><a href='http://arxiv.org/abs/2510.22257v1'>LUNA: Efficient and Topology-Agnostic Foundation Model for EEG Signal Analysis</a></h3>
      <div class='meta'>2025-10-25 · Berkay Döner, Thorir Mar Ingolfsson, Luca Benini, Yawei Li</div>
      <p><strong>Summary Highlights:</strong></p>
      <ul class='summary-points'><li>New EEG foundation model: LUNA unifies arbitrary electrode layouts into a fixed latent space using learned queries and cross-attention.</li><li>Topology-agnostic and efficient: Decouples computation from channel count, scaling linearly and reducing FLOPs by 300x and memory by 10x.</li><li>Strong transfer performance: Achieves state-of-the-art results on TUAR (0.921 AUROC) and TUSL while generalizing across diverse electrode configurations.</li></ul>
      <p><strong>Unique contribution:</strong> LUNA introduces a topology-invariant encoder using learned queries and cross-attention to map arbitrary electrode layouts into a fixed latent space, enabling efficient, montage-agnostic EEG modeling with linear-in-channels complexity.</p>
      <details class='summary-detail'><summary>Detailed summary</summary><p>LUNA addresses the challenge of topological heterogeneity in EEG by introducing a learned query-based encoder that projects variable-channel inputs into a fixed-size latent representation. This design decouples computational cost from electrode count, allowing linear scaling with channels rather than quadratic. Pre-trained on over 21,000 hours of multi-montage EEG data using masked-patch reconstruction, LUNA transfers effectively to four downstream tasks—abnormality detection, artifact rejection, slowing classification, and emotion recognition—achieving state-of-the-art results on TUAR and TUSL while reducing FLOPs by 300x and GPU memory by up to 10x.</p></details>
      <p class='chips'><span class='chip chip-paper_type' title='paper type'>New Model</span> <span class='chip chip-backbone' title='backbone'>Transformer</span> <span class='chip chip-objective' title='objective'>Masked Reconstruction</span> <span class='chip chip-tokenization' title='tokenization'>Time Patch</span> <span class='chip chip-topology' title='topology'>Topology Agnostic</span></p>
      <p><a href='https://github.com/pulp-bio/BioFoundation'>code</a> </p>
    </article>
    

    <article class='paper-card' id='2510.27522'>
      <h3><a href='http://arxiv.org/abs/2510.27522v1'>Leveraging Generic Time Series Foundation Models for EEG Classification</a></h3>
      <div class='meta'>2025-10-31 · Théo Gnassounou, Yessin Moakher, Shifeng Xie, Vasilii Feofanov, Ievgen Redko</div>
      <p><strong>Summary Highlights:</strong></p>
      <ul class='summary-points'><li>New EEG foundation model approach: Shows that Mantis, a general time series foundation model, can effectively transfer to EEG tasks without extensive domain-specific pretraining.</li><li>Strong empirical evidence: Mantis consistently outperforms both EEGNet baseline and CBraMod (EEG-specific foundation model) across motor imagery and sleep staging tasks.</li><li>Synthetic pretraining validation: Mantis pretrained solely on synthetic data achieves state-of-the-art performance on EEG tasks, suggesting synthetic data can effectively replace real EEG pretraining.</li></ul>
      <p><strong>Unique contribution:</strong> Demonstrates that a general-purpose time series foundation model pretrained on synthetic or heterogeneous data can outperform specialized EEG models on multiple EEG classification tasks.</p>
      <details class='summary-detail'><summary>Detailed summary</summary><p>This paper investigates whether general-purpose time series foundation models can effectively transfer to EEG classification tasks without extensive domain-specific pretraining. The authors evaluate Mantis, a recently proposed time series classification foundation model, on two EEG tasks: motor imagery classification and sleep staging. They compare Mantis against EEGNet, a widely used convolutional baseline, and CBraMod, the most recent EEG-specific foundation model. The experiments reveal that Mantis, even when pretrained on purely synthetic data or heterogeneous time series from non-neural domains, consistently outperforms both baselines across multiple datasets. Notably, Mantis achieves competitive results with CBraMod on motor imagery tasks while significantly surpassing it on sleep staging, particularly in scenarios with limited spatial information. These findings suggest that generalist time series foundation models can effectively transfer to EEG analysis, potentially reducing the need for extensive domain-specific pretraining.</p></details>
      <p class='chips'><span class='chip chip-paper_type' title='paper type'>New Model</span> <span class='chip chip-backbone' title='backbone'>Transformer</span> <span class='chip chip-objective' title='objective'>Contrastive</span> <span class='chip chip-tokenization' title='tokenization'>Time Patch</span> <span class='chip chip-topology' title='topology'>Channel Flexible</span></p>
      <p> <a href='Mantis checkpoints available (real and synthetic pretraining)'>weights</a></p>
    </article>
    </section>
  </main>
  <script src='../../assets/site.js'></script>
</body></html>
